{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Homework 5 可解释模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- 基本变量 ---------------------------------\n",
    "checkpoint_path = \"\"\n",
    "train_data_path = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------- 被解释模型定义 ---------------------------------\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),  # 64 * 128 * 128\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  # 64 * 64 * 64\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),  # 128 * 64 * 64\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),   # 128 * 32 * 32\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),  # 256,32,32\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),    # 256, 16,16\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),  # 512,8,8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),    # 512,8,8\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),  # 512,8,8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0)  # 512, 4,4\n",
    "        )\n",
    "        self.fn = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------- 加载模型 ---------------------------------\n",
    "device = \"cpu\"\n",
    "model = Classifier()\n",
    "model = model.to(device)\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "# 加载模型参数\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------- 定义数据集 ---------------------------------\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X\n",
    "            \n",
    "    def getbatch(self, indices):\n",
    "        images = []\n",
    "        labels = []\n",
    "        for index in indices:\n",
    "          image, label = self.__getitem__(index)\n",
    "          images.append(image)\n",
    "          labels.append(label)\n",
    "        return torch.stack(images), torch.tensor(labels)\n"
   ]
  },
  {
   "source": [
    "## Saliency Map\n",
    "\n",
    "一般情况下，我们改变Model parameter 来拟合 image 与Label ，所以loss 在计算 backward时，我们只在乎loss 对 model parameter 的偏微分。\n",
    "\n",
    "但是从数值上看，image 本身也是一个连续tensor，所以我们可以计算loss 对 input image的偏微分。这个偏微分表示，在model 和parameter 固定不变的情况下，改变image的某个像素 pixel value 会对loss 产生什么样的影响。\n",
    "\n",
    "习惯上，把loss变化剧烈程度解释为当前pixel的重要度。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    return (image-image.min())/(image.max() - image.min())\n",
    "\n",
    "def compute_saliency_maps(x,y,model):\n",
    "    model.eval()\n",
    "    X = x.to(device)\n",
    "    Y = y.to(device)\n",
    "\n",
    "    # 使得X具有梯度，因为我们要计算 loss 对 input部分的微分\n",
    "    X.requires_grad_()\n",
    "\n",
    "    Y_hat = model(X)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(Y_hat,Y)\n",
    "    # 计算 loss 对 X 的偏微分\n",
    "    loss.backward()\n",
    "\n",
    "    salienies = X.grad.abs().detach().cpu()\n",
    "    # 不同图片的Gradient 可能有很大落差，第一张图片Gradient 在100-10000 之间，而第二张图片Gradient 在0.001 - 0.01 之间，这就造成如果我们使用同样的色阶画图，第一张图片就会非常亮，而第二张图片非常暗。所以对每张saliency 做norma\n",
    "    salienies = torch.stack([normalize(item) for item in salienies])\n",
    "    return salienies\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_indices = [83, 100, 750,500]\n",
    "# 从数据集中获取 Input 和 label\n",
    "images, labels = train_set.getbatch(img_indices)\n",
    "saliencies = compute_saliency_maps(images,labels,model)\n",
    "\n",
    "# 利用 matplatlib 绘图\n",
    "ig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8)) \n",
    "for row,target in enumerate([images,saliencies]):\n",
    "    for column ,img in enumerate(target):\n",
    "        axs[row][column].imshow(img.permute(1,2,0),numpy())\n",
    "        # image tensor dimension是 channel,height,width\n",
    "        # 而 matplatlib 需要的形状为：height,width,channels\n",
    "plt.show()    \n"
   ]
  },
  {
   "source": [
    "## Filter Explanation\n",
    "如果想知道某个 filter 到底有什么作用，我们需要做两件事情\n",
    "* Filter activation： 选几张图片，看看图片中有哪些位置会 activate 当前 filter\n",
    "* Filter visualization: 怎样的图片，可以最大程度的 activate 当前  Filter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_activations = None\n",
    "def filter_explanation(x,model,filter_id,iteration = 100,lr = 1):\n",
    "    model.eval()\n",
    "    def hook(modek,input,output):\n",
    "        global layer_activations\n",
    "        layer_activations = output\n",
    "    \n",
    "    model.cnn[cnn_id].register_forsward_hook(hook)\n",
    "    # 告诉pytorch 当 forward 经过 filter_id 时，先呼叫hook后，才可以继续 forward\n",
    "    \n",
    "    model(x.to(device))\n",
    "\n",
    "    filter_activations = layer_activations[:,filter_id,:,:].detach().cpu()\n",
    "\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "filter_visualization = x.detach().cpu().squeeze()[0]\n",
    "  hook_handle.remove()\n"
   ]
  },
  {
   "source": [
    "## Lime Explanation\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input):\n",
    "    model.eval()\n",
    "    # batch channel height width\n",
    "    input = torch.FloatTensor(input).permute(0,3,1,2)\n",
    "    output = model(input.to(device))\n",
    "    return output.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "def segmentation(input):\n",
    "    # 利用 skimage 提供的 segmentation 将图片分成100分\n",
    "    return slic(input,n_segments = 100,compactness=1,sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                  \n",
    "img_indices = [83, 4218, 4707, 8598]\n",
    "images, labels = train_set.getbatch(img_indices)\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 8))                                                                                                                                                                 \n",
    "np.random.seed(16)                                                                                                                                                       \n",
    "# 讓實驗 reproducible\n",
    "for idx, (image, label) in enumerate(zip(images.permute(0, 2, 3, 1).numpy(), labels)):                                                                                                                                             \n",
    "    x = image.astype(np.double)\n",
    "    # lime 這個套件要吃 numpy array\n",
    "\n",
    "    explainer = lime_image.LimeImageExplainer()                                                                                                                              \n",
    "    explaination = explainer.explain_instance(image=x, classifier_fn=predict, segmentation_fn=segmentation)\n",
    "    # 基本上只要提供給 lime explainer 兩個關鍵的 function，事情就結束了\n",
    "    # classifier_fn 定義圖片如何經過 model 得到 prediction\n",
    "    # segmentation_fn 定義如何把圖片做 segmentation\n",
    "\n",
    "    lime_img, mask = explaination.get_image_and_mask(                                                                                                                         \n",
    "                                label=label.item(),                                                                                                                           \n",
    "                                positive_only=False,                                                                                                                         \n",
    "                                hide_rest=False,                                                                                                                             \n",
    "                                num_features=11,                                                                                                                              \n",
    "                                min_weight=0.05                                                                                                                              \n",
    "                            )\n",
    "    # 把 explainer 解釋的結果轉成圖片\n",
    "    \n",
    "    axs[idx].imshow(lime_img)\n",
    "\n",
    "plt.show()\n",
    "# 從以下前三章圖可以看到，model 有認出食物的位置，並以該位置為主要的判斷依據\n",
    "# 唯一例外是第四張圖，看起來 model 似乎比較喜歡直接去認「碗」的形狀，來判斷該圖中屬於 soup 這個 class\n",
    "# 至於碗中的內容物被標成紅色，代表「單看碗中」的東西反而有礙辨認。\n",
    "# 當 model 只看碗中黃色的一坨圓形，而沒看到「碗」時，可能就會覺得是其他黃色圓形的食物。"
   ]
  }
 ]
}